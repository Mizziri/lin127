{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmco75APQEOg",
        "outputId": "1110b5bd-3ed2-40ef-ea34-4fa312c7eaec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296181 sha256=3d1b9612e1a30bc281de49d211052cab94d81a8a2d24a12592e36e0163cf09ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import re\n",
        "import itertools\n",
        "import random\n",
        "import statistics\n",
        "import time\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from functools import reduce\n",
        "\n",
        "import numpy as np\n",
        "import fasttext\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize.sonority_sequencing import SyllableTokenizer\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "9ic5UyLyPq8k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9dsmUffPPiR5"
      },
      "outputs": [],
      "source": [
        "def get_swda(py_url, zip_url, subdir):\n",
        "    # Ensure `swda.py` exists\n",
        "    py_filename = \"swda.py\"\n",
        "    if not os.path.exists(py_filename):\n",
        "        print(f'{py_filename} not found. Downloading from {py_url}...')\n",
        "        response = requests.get(py_url)\n",
        "        with open(py_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "    # Ensure the zip exists. If 'swda/swda-metadata.csv' does not exist, then\n",
        "    # this indicates the zip has not been extracted, so unzip it now:\n",
        "    if not os.path.exists(os.path.join(subdir, 'swda-metadata.csv')):\n",
        "        if not os.path.exists(subdir):\n",
        "            os.makedirs(subdir)\n",
        "        zip_filename = \"swda.zip\"\n",
        "        if not os.path.exists(zip_filename):\n",
        "            print(f'{zip_filename} not found. Downloading from {zip_url}...')\n",
        "            response = requests.get(zip_url)\n",
        "            with open(zip_filename, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "        # Extract zip file into subdir\n",
        "        print(f'Extracting {zip_filename} into {subdir}...')\n",
        "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall()\n",
        "\n",
        "def setup():\n",
        "    data_dir = \"swda\"\n",
        "\n",
        "    # Download `swda.py` and `swda.zip` if necessary\n",
        "    py_url = \"https://github.com/cgpotts/swda/raw/master/swda.py\"\n",
        "    zip_url = \"https://github.com/cgpotts/swda/raw/master/swda.zip\"\n",
        "    get_swda(py_url, zip_url, data_dir)\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n",
        "    nltk.download('wordnet')\n",
        "    try:\n",
        "      nltk.data.find('tokenizers/punkt_tab')\n",
        "    except LookupError:\n",
        "      nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "setup()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utFFprYQQDVQ",
        "outputId": "3d23cffe-35a2-419c-c48f-3094cfa6477f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swda.py not found. Downloading from https://github.com/cgpotts/swda/raw/master/swda.py...\n",
            "swda.zip not found. Downloading from https://github.com/cgpotts/swda/raw/master/swda.zip...\n",
            "Extracting swda.zip into swda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regex_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "syllable_tokenizer = SyllableTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "DATA_DIR = \"swda\"\n",
        "COUNT = 0\n",
        "TRAIN_MALE_ENTRIES = 0\n",
        "TRAIN_FEMALE_ENTRIES = 0\n",
        "TRAIN_DISCARDED_ENTRIES = 0\n",
        "VAL_MALE_ENTRIES = 0\n",
        "VAL_FEMALE_ENTRIES = 0\n",
        "VAL_DISCARDED_ENTRIES = 0\n",
        "\n",
        "wordcounts = defaultdict(float)\n",
        "male_wordcounts = defaultdict(float)\n",
        "female_wordcounts = defaultdict(float)\n",
        "\n",
        "DIVISIVE_WORDS = ['husband', 'wonderful', 'wear', 'dress', 'wife', 'huhuh', 'goodness',\n",
        "                  'dinner', 'tax', 'cook', 'woman', 'gosh', 'color', 'girl', 'god',\n",
        "                  'neat', 'mother', 'oh', 'vacation', 'flower', 'told', 'men', 'fish',\n",
        "                  'ours', 'university', 'love',]\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)  # Default to NOUN if unknown\n",
        "\n",
        "def get_word_lemma_counts(sent):\n",
        "    out = defaultdict(float)\n",
        "\n",
        "    # Get rid of all annotations, make lowercase, and nuke punctuation\n",
        "    sent = purge_enclosed(sent)\n",
        "    sent = \" \".join(clean_square_brackets(sent).split())\n",
        "    sent = sent.lower()\n",
        "    sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    words = nltk.word_tokenize(sent)\n",
        "\n",
        "    for word in words:\n",
        "        pos = get_wordnet_pos(word)\n",
        "        lemma = lemmatizer.lemmatize(word, pos=pos)\n",
        "        out[f'{lemma}'] += 1\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def clean_square_brackets(text):\n",
        "    stack = []\n",
        "    result = \"\"\n",
        "    i = 0\n",
        "\n",
        "    while i < len(text):\n",
        "        if text[i] == \"[\":\n",
        "            # Start of a new bracketed phrase, push current result to stack\n",
        "            stack.append(result)\n",
        "            result = \"\"\n",
        "            i += 1\n",
        "        elif text[i] == \"]\":\n",
        "            # End of a bracketed phrase, pop from stack\n",
        "            if stack:\n",
        "                before_bracket = stack.pop()\n",
        "                # Split the current result by '+', and take the part after '+'\n",
        "                parts = result.split(\"+\")\n",
        "                if len(parts) > 1:\n",
        "                    result = before_bracket + parts[1].strip()\n",
        "                else:\n",
        "                    result = before_bracket + result\n",
        "            i += 1\n",
        "        else:\n",
        "            # Accumulate characters in the current result\n",
        "            result += text[i]\n",
        "            i += 1\n",
        "\n",
        "    return result.strip()\n",
        "\n",
        "def purge_enclosed(text):\n",
        "    stack = []\n",
        "    result = []\n",
        "    braces = {\"(\": \")\", \"{\": \"}\", \"<\": \">\"}\n",
        "    open_braces = set(braces.keys())\n",
        "    close_braces = set(braces.values())\n",
        "\n",
        "    for char in text:\n",
        "        if char in open_braces:\n",
        "            # Start of a new enclosure, push to stack\n",
        "            stack.append(char)\n",
        "        elif char in close_braces:\n",
        "            # End of an enclosure, pop from stack\n",
        "            if stack and braces[stack[-1]] == char:\n",
        "                stack.pop()\n",
        "        elif not stack:\n",
        "            # Only add to result if we're not inside any braces\n",
        "            result.append(char)\n",
        "\n",
        "    return ''.join(result).strip()\n",
        "\n",
        "def side_to_sentences(text):\n",
        "    # Convert tuple into paragraph\n",
        "    text = ''.join(text)\n",
        "\n",
        "    # Get sentences\n",
        "    sentences = nltk.tokenize.sent_tokenize(text)\n",
        "    # nltk doesn't recognize \"/\" as a sentence divider, so divide it manually\n",
        "    split_at_slashes = []\n",
        "    for sent in sentences:\n",
        "        split_at_slashes.extend(sent.split(\"/\"))\n",
        "\n",
        "    result = \"\"\n",
        "    # Go through and destroy anything too short, and clean up the others\n",
        "    for sent in split_at_slashes:\n",
        "        if len(sent) <= 8:\n",
        "            continue\n",
        "        sent = purge_enclosed(sent)\n",
        "        sent = \" \".join(clean_square_brackets(sent).split())\n",
        "        sent = sent.lower()\n",
        "        result += sent + \" \"\n",
        "\n",
        "    sent = sent.strip('#')\n",
        "\n",
        "    return result, len(split_at_slashes)"
      ],
      "metadata": {
        "id": "I-B-Kc0mQkC-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN FASTTEXT FEATURES ARE DEFINED HERE"
      ],
      "metadata": {
        "id": "mZ-pap6TQ_50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_partial_conversation(features, tuple_of_sentences):\n",
        "    sentences, num_sentences = side_to_sentences(tuple_of_sentences)\n",
        "\n",
        "    tokens = nltk.word_tokenize(sentences)\n",
        "    # nontrivial_tokens = [token for token in tokens if len(token) > 1]\n",
        "\n",
        "    lemmata = []\n",
        "\n",
        "    for word in tokens:\n",
        "        pos = get_wordnet_pos(word)\n",
        "        lemmata.append(lemmatizer.lemmatize(word, pos=pos))\n",
        "\n",
        "    ## Very effective: 55.14%\n",
        "    for word in DIVISIVE_WORDS:\n",
        "        if word in lemmata:\n",
        "            features[f'has_{word}'] = \"True\"\n",
        "        else:\n",
        "            features[f'has_{word}'] = \"False\"\n",
        "\n",
        "    ## Not very effective: 50.26%\n",
        "    features['interrupted'] = \"-\" in sentences\n",
        "\n",
        "\n",
        "\n",
        "    ## Not very effective: 50.27%\n",
        "    # features['mentions_children'] = reduce(lambda a, b: a or b,\n",
        "    #                            [word in lemmata for word in [\"child\",\n",
        "    #                                                       \"kid\",\n",
        "    #                                                       \"son\",\n",
        "    #                                                       \"daughter\",\n",
        "    #                                                       \"baby\",\n",
        "    #                                                       ]])\n",
        "\n",
        "    ## No effect\n",
        "    # avg_sentence_length = len(tokens) / num_sentences\n",
        "    # if avg_sentence_length < 7:\n",
        "    #     features['avg_length'] = \"short\"\n",
        "    # elif avg_sentence_length < 14:\n",
        "    #     features['avg_length'] = \"medium\"\n",
        "    # else:\n",
        "    #     features['avg_length'] = \"long\"\n",
        "\n",
        "\n",
        "    ### Pronoun features: no effect on accuracy\n",
        "    # # Count pronouns, loop is faster than .count here\n",
        "    # num_prons, fp_prons, sp_prons = 0,0,0\n",
        "    # male_tp_prons, female_tp_prons, neutral_tp_prons = 0,0,0\n",
        "    # for word in tokens:\n",
        "    #     if word == 'i':\n",
        "    #         fp_prons += 1\n",
        "    #         num_prons += 1\n",
        "    #     elif word == 'you':\n",
        "    #         sp_prons += 1\n",
        "    #         num_prons += 1\n",
        "    #     elif word in ['he', 'him', 'his']:\n",
        "    #         male_tp_prons += 1\n",
        "    #         num_prons += 1\n",
        "    #     elif word in ['she', 'her', 'hers']:\n",
        "    #         female_tp_prons += 1\n",
        "    #         num_prons += 1\n",
        "    #     elif word in ['they', 'them', 'theirs']:\n",
        "    #         neutral_tp_prons += 1\n",
        "    #         num_prons += 1\n",
        "\n",
        "    # tp_prons = male_tp_prons + female_tp_prons + neutral_tp_prons\n",
        "\n",
        "    # features['primary_fp_vs_sp_prons'] = \"na\"\n",
        "    # features['primary_person_prons'] = \"na\"\n",
        "    # features['primary_gender_prons'] = \"na\"\n",
        "\n",
        "    ## No effect on accuracy\n",
        "    # if fp_prons > sp_prons:\n",
        "    #     features['primary_fp_vs_sp_prons'] = \"fp\"\n",
        "    #     if fp_prons > tp_prons:\n",
        "    #         features['primary_person_prons'] = \"fp\"\n",
        "    #     elif tp_prons > fp_prons:\n",
        "    #         features['primary_person_prons'] = \"tp\"\n",
        "    # elif sp_prons > fp_prons:\n",
        "    #     features['primary_fp_vs_sp_prons'] = \"sp\"\n",
        "    #     if sp_prons > tp_prons:\n",
        "    #         features['primary_person_prons'] = \"sp\"\n",
        "    #     elif tp_prons > sp_prons:\n",
        "    #         features['primary_person_prons'] = \"tp\"\n",
        "\n",
        "    ## Zero effect on accuracy\n",
        "    # if neutral_tp_prons > male_tp_prons:\n",
        "    #     if neutral_tp_prons > female_tp_prons:\n",
        "    #         features['primary_gender_prons'] = \"neutral\"\n",
        "    #     elif female_tp_prons > neutral_tp_prons:\n",
        "    #         features['primary_gender_prons'] = \"female\"\n",
        "    # elif male_tp_prons > neutral_tp_prons:\n",
        "    #     if male_tp_prons > female_tp_prons:\n",
        "    #         features['primary_gender_prons'] = \"male\"\n",
        "    #     elif female_tp_prons > male_tp_prons:\n",
        "    #         features['primary_gender_prons'] = \"female\"\n",
        "\n",
        "    ## Analyze word lengths slightly more than naively\n",
        "    # word_lens = list(map(len, nontrivial_tokens))\n",
        "\n",
        "    ## This should account for skewed data where someone uses lots of small words\n",
        "    # if len(word_lens) > 0:\n",
        "    #     avg_word_length = statistics.mean(word_lens) / num_sentences\n",
        "    # else:\n",
        "    #     return None\n",
        "\n",
        "    ## Zero change to accuracy\n",
        "    # if avg_word_length >= 6:\n",
        "    #     features['avg_word_length'] = \"long\"\n",
        "    # elif avg_word_length >= 3:\n",
        "    #     features['avg_word_length'] = \"medium\"\n",
        "    # else:\n",
        "    #     features['avg_word_length'] = \"short\"\n",
        "\n",
        "\n",
        "    ## Zero change to accuracy\n",
        "    ## This should be more representative of medium-length words\n",
        "    # nontrivial_word_lens = [n / num_sentences for n in word_lens if n > 3]\n",
        "    # if len(nontrivial_word_lens) > 0:\n",
        "    #     median_nontrivial_word_length = statistics.median(nontrivial_word_lens)\n",
        "    # else:\n",
        "    #     median_nontrivial_word_length = 0\n",
        "\n",
        "    # if median_nontrivial_word_length >= 7:\n",
        "    #     features['median_nontrivial_word_length'] = \"long\"\n",
        "    # elif median_nontrivial_word_length >= 5:\n",
        "    #     features['median_nontrivial_word_length'] = \"medium\"\n",
        "    # else:\n",
        "    #     features['median_nontrivial_word_length'] = \"short\"\n",
        "\n",
        "\n",
        "    ## about ~16% of english words are 8 or longer characters\n",
        "\n",
        "    # features['has_long_word_by_chars'] = \"False\"\n",
        "    # for n in nontrivial_word_lens:\n",
        "    #     if n >= 8:\n",
        "    #         features['has_long_word_by_chars'] = \"True\"\n",
        "    #         break\n",
        "\n",
        "    # Analyze syllables, turn each word into the number of syllables it contains\n",
        "    # This has some small issues with apostrophes, since we split on them before.\n",
        "    syllable_counts = [len(syllable_tokenizer.tokenize(word)) for word in tokens]\n",
        "\n",
        "    # avg_sent_syllables = sum(syllable_counts) / num_sentences\n",
        "\n",
        "    # No change to accuracy\n",
        "    # if avg_sent_syllables > 30:\n",
        "    #     features['avg_sent_syllables'] = \"many\"\n",
        "    # elif avg_sent_syllables > 15:\n",
        "    #     features['avg_sent_syllables'] = \"medium\"\n",
        "    # else:\n",
        "    #     features['avg_sent_syllables'] = \"few\"\n",
        "\n",
        "    # if len(syllable_counts) > 0:\n",
        "    #     avg_num_syllables = statistics.mean(syllable_counts) / num_sentences\n",
        "    # else:\n",
        "    #     return None\n",
        "\n",
        "    # nontrivial_syllable_counts = [n for n in syllable_counts if n > 1]\n",
        "\n",
        "    # if len(nontrivial_syllable_counts) > 0:\n",
        "    #     median_nontrivial_syllables = statistics.median(nontrivial_syllable_counts)\n",
        "    # else:\n",
        "    #     median_nontrivial_syllables = 0\n",
        "\n",
        "    # if median_nontrivial_syllables > 6:\n",
        "    #     features['median_nontrivial_syllables'] = \"many\"\n",
        "    # elif median_nontrivial_syllables >= 3:\n",
        "    #     features['median_nontrivial_syllables'] = \"medium\"\n",
        "    # else:\n",
        "    #     features['median_nontrivial_syllables'] = \"few\"\n",
        "\n",
        "    # features['has_3_syllable_or_longer'] = \"False\"\n",
        "    # features['has_4_syllable_or_longer'] = \"False\"\n",
        "    # features['has_5_syllable_or_longer'] = \"False\"\n",
        "\n",
        "    for n in syllable_counts:\n",
        "        if n >= 5:\n",
        "            features['has_3_syllable_or_longer'] = \"True\"\n",
        "            features['has_4_syllable_or_longer'] = \"True\"\n",
        "            features['has_5_syllable_or_longer'] = \"True\"\n",
        "            break\n",
        "        if n >= 4:\n",
        "            features['has_3_syllable_or_longer'] = \"True\"\n",
        "            features['has_4_syllable_or_longer'] = \"True\"\n",
        "        if n >= 3:\n",
        "            features['has_3_syllable_or_longer'] = \"True\"\n",
        "\n",
        "    # Create the formatted FastText line\n",
        "    formatted = f\"__label__{features['sex']} \"\n",
        "    for feature in features.keys():\n",
        "        if feature == 'sex':    # Already did this one as __label__ for FastText\n",
        "            continue\n",
        "        formatted += f\"{feature}:{str(features[feature])} \"\n",
        "    formatted += f'partial_conversation:\"{tuple_of_sentences}\"'\n",
        "    return formatted"
      ],
      "metadata": {
        "id": "d5bwmCpsQtRo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_partial_conversations(output_file, transcript, mode):\n",
        "    metadata = transcript.metadata\n",
        "    idx = transcript.conversation_no\n",
        "\n",
        "    for caller in [\"A\", \"B\"]:\n",
        "        features = defaultdict(str)\n",
        "        features['sex'] = (metadata[idx][\"from_caller_sex\"] if caller == \"A\"\n",
        "                            else metadata[idx][\"to_caller_sex\"])\n",
        "\n",
        "        # Get all sentences spoken by caller A\n",
        "        interrupts = False\n",
        "        side = []\n",
        "        utt_features_to_check = [\n",
        "            # 'bh', # no effect\n",
        "            'ba', # huge effect: 56.35% accuracy\n",
        "            # '^g', # no effect\n",
        "            # 'fa', # no effect\n",
        "            # 'ft', # no effect\n",
        "            'ar', # slight effect: 50.50%\n",
        "            'nn', # slight effect: 50.82% accuracy\n",
        "            # 'ng', # no effect\n",
        "            # 'bf', # no effect\n",
        "            ]\n",
        "        for utt in transcript.utterances:\n",
        "            if utt.caller == caller:\n",
        "                side.append(utt)\n",
        "                for feat in utt_features_to_check:\n",
        "                    if utt.act_tag == feat:\n",
        "                        features[feat] = True\n",
        "            elif not interrupts:\n",
        "                if \"-\" in utt.text:\n",
        "                    interrupts = True\n",
        "\n",
        "        for feat in utt_features_to_check:\n",
        "            if not features[feat]:\n",
        "                features[feat] = False\n",
        "\n",
        "        GRAM_LENGTH = 50\n",
        "        global COUNT\n",
        "\n",
        "        # Use itertools.combinations to get every possible combination of 10\n",
        "        # sentences spoken by caller A\n",
        "        sentence_ngrams = []\n",
        "        utterance_strings = [utt.text for utt in side]\n",
        "        if len(side) < GRAM_LENGTH:\n",
        "            COUNT += 1\n",
        "\n",
        "            sentence_ngrams = [utterance_strings]\n",
        "        else:\n",
        "            sentence_ngrams = [random.sample(utterance_strings, GRAM_LENGTH) for _ in range(int(3 * (len(side) // GRAM_LENGTH)))]\n",
        "\n",
        "        # Pre-count total number of each sex's entry\n",
        "        if mode == 'TRAIN':\n",
        "            global TRAIN_MALE_ENTRIES\n",
        "            global TRAIN_FEMALE_ENTRIES\n",
        "            global TRAIN_DISCARDED_ENTRIES\n",
        "            for sentence_ngram in sentence_ngrams:\n",
        "                if features['sex'] == 'FEMALE':\n",
        "                    TRAIN_FEMALE_ENTRIES += 1\n",
        "                elif features['sex'] == 'MALE':\n",
        "                    TRAIN_MALE_ENTRIES += 1\n",
        "        elif mode == 'VAL':\n",
        "            global VAL_MALE_ENTRIES\n",
        "            global VAL_FEMALE_ENTRIES\n",
        "            global VAL_DISCARDED_ENTRIES\n",
        "            for sentence_ngram in sentence_ngrams:\n",
        "                if features['sex'] == 'FEMALE':\n",
        "                    VAL_FEMALE_ENTRIES += 1\n",
        "                elif features['sex'] == 'MALE':\n",
        "                    VAL_MALE_ENTRIES += 1\n",
        "\n",
        "\n",
        "        # Manually calculated: 1 - F/M, where F and M are the total numbers of\n",
        "        # sentence ngrams, respectively\n",
        "        TRAIN_MOST_COMMON_SEX = 'FEMALE'\n",
        "        VAL_MOST_COMMON_SEX = 'FEMALE'\n",
        "        TRAIN_DISCARD_PROBABILITY = 1 - (10910.0 / 14310.0)\n",
        "        VAL_DISCARD_PROBABILITY = 1 - (4685 / 6987)\n",
        "\n",
        "        features['interrupts'] = str(interrupts)\n",
        "\n",
        "        # Process every combination of GRAM_LENGTH sentences\n",
        "        for sentence_ngram in sentence_ngrams:\n",
        "            sentence_lemmas = get_word_lemma_counts(sentence_ngram)\n",
        "            for lemma, count in sentence_lemmas.items():\n",
        "                wordcounts[lemma] += count\n",
        "                if features[\"sex\"] == 'MALE':\n",
        "                    male_wordcounts[lemma] += count\n",
        "                elif features[\"sex\"] == 'FEMALE':\n",
        "                    female_wordcounts[lemma] += count\n",
        "            # Discard all excess female entries to balance training data\n",
        "            if mode == \"TRAIN\" and features[\"sex\"] == TRAIN_MOST_COMMON_SEX:\n",
        "                if np.random.random() < TRAIN_DISCARD_PROBABILITY:\n",
        "                    TRAIN_DISCARDED_ENTRIES += 1\n",
        "                    continue\n",
        "            if mode == \"VAL\" and features[\"sex\"] == VAL_MOST_COMMON_SEX:\n",
        "                if np.random.random() < VAL_DISCARD_PROBABILITY:\n",
        "                    VAL_DISCARDED_ENTRIES += 1\n",
        "                    continue\n",
        "\n",
        "\n",
        "            formatted = format_partial_conversation(features, sentence_ngram)\n",
        "            if formatted:\n",
        "                output_file.write(formatted + \"\\n\")\n",
        "\n",
        "def make_fasttext(subdirs, output_file, Transcript, mode):\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        # Step 1: Process each specified subdirectory\n",
        "        for subdir_path in subdirs:\n",
        "            if not os.path.isdir(subdir_path):\n",
        "                print(f\"Skipping invalid directory {subdir_path}\")\n",
        "                continue\n",
        "\n",
        "            # Step 2: Iterate over .utt.csv files\n",
        "            print(f\"Processing files in {subdir_path}...\")\n",
        "            for filename in os.listdir(subdir_path):\n",
        "                if filename.endswith(\".utt.csv\"):\n",
        "                    filepath = os.path.join(subdir_path, filename)\n",
        "\n",
        "                    # Step 3: Create a Transcript object\n",
        "                    try:\n",
        "                        metadata_file = os.path.join(DATA_DIR, \"swda-metadata.csv\")\n",
        "                        transcript = Transcript(filepath, metadata_file)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing file {filepath}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    add_partial_conversations(outfile, transcript, mode = mode)\n",
        "\n",
        "    print(f\"Saved sentences and conversation sides to {output_file}\")\n",
        "\n",
        "def validate(model, VALIDATION_DIRS, Transcript):\n",
        "    # Preprocess the validation data\n",
        "    validation_ft = \"validation.ft\"\n",
        "    VALIDATION_DIRS = [os.path.join(DATA_DIR, filename) for filename in VALIDATION_DIRS]\n",
        "    print(f\"VALIDATION_DIRS: {VALIDATION_DIRS}\")\n",
        "    make_fasttext(VALIDATION_DIRS, validation_ft, Transcript, mode = \"VAL\")\n",
        "\n",
        "    # Measure performance on validation set\n",
        "    validation_performance = model.test('validation.ft')\n",
        "    print(f\"Performance on validation set \"\n",
        "          f\"({validation_performance[0]} entries): \"\n",
        "          f\"\\t{validation_performance[1]*100:.2f}% accuracy\")\n",
        "\n",
        "def test(model, TEST_DIRS, Transcript):\n",
        "    # Preprocess the test data\n",
        "    test_ft = \"test.ft\"\n",
        "    TEST_DIRS = [os.path.join(DATA_DIR, filename) for filename in TEST_DIRS]\n",
        "    print(f\"TEST_DIRS: {TEST_DIRS}\")\n",
        "    make_fasttext(TEST_DIRS, test_ft, Transcript, mode = \"TEST\")\n",
        "\n",
        "    # Measure performance on test set\n",
        "    test_performance = model.test('test.ft')\n",
        "    print(f\"Performance on test set \"\n",
        "          f\"({test_performance[0]} entries): \"\n",
        "          f\"\\t{test_performance[1]*100:.2f}% accuracy\")\n",
        "\n",
        "def train():\n",
        "    start = time.time()\n",
        "\n",
        "    TRAIN_DIRS = []\n",
        "    VALIDATION_DIRS = []\n",
        "    TEST_DIRS = []\n",
        "    for i in range(0, 13):\n",
        "        filename = f\"sw{i:02}utt\"\n",
        "        if i < 7:#< 7:\n",
        "            TRAIN_DIRS.append(filename)\n",
        "        elif i < 10:#< 10:\n",
        "            VALIDATION_DIRS.append(filename)\n",
        "        else:\n",
        "            TEST_DIRS.append(filename)\n",
        "\n",
        "    # The import needs to be in this function, not in the root namespace,\n",
        "    # because `setup` needs to import train() without knowing what `Transcript`\n",
        "    # is\n",
        "    from swda import Transcript\n",
        "\n",
        "    # Preprocess the training data: Combine files in TRAIN_DIRS into one big\n",
        "    # training file in FastText format\n",
        "    train_ft = \"train.ft\"\n",
        "    TRAIN_DIRS = [os.path.join(DATA_DIR, filename) for filename in TRAIN_DIRS]\n",
        "    print(f\"TRAIN_DIRS: {TRAIN_DIRS}\")\n",
        "    make_fasttext(TRAIN_DIRS, train_ft, Transcript, mode = \"TRAIN\")\n",
        "\n",
        "    print(f\"{TRAIN_MALE_ENTRIES} male partial conversations in training\")\n",
        "    print(f\"{TRAIN_FEMALE_ENTRIES} female partial conversations in training\")\n",
        "    print(f\"{TRAIN_DISCARDED_ENTRIES} discarded partial conversations in training\")\n",
        "\n",
        "    word_disparities = defaultdict(float)\n",
        "\n",
        "    print(\"Total words: \" + str(sum(wordcounts.values())))\n",
        "    print(\"Unique words: \" + str(len(wordcounts.keys())))\n",
        "\n",
        "    words_filtered = 0\n",
        "\n",
        "    for lemma, count in wordcounts.items():\n",
        "        # 72 is about 0.01% of the corpus\n",
        "        if count > 72:\n",
        "            disparity = abs((male_wordcounts[lemma]/.545)\n",
        "                            - (female_wordcounts[lemma]/.455)) / count\n",
        "            word_disparities[lemma] = disparity\n",
        "        else:\n",
        "            words_filtered += 1\n",
        "\n",
        "    print(\"Words remaining: \" + str(len(word_disparities.keys())))\n",
        "\n",
        "    print(\"Words filtered: \" + str(words_filtered))\n",
        "\n",
        "    top_divisive_words = dict(sorted(word_disparities.items(),\n",
        "                         key=lambda x: x[1],\n",
        "                         reverse=True)[:30]).keys()\n",
        "    print(\"Most divisive words: \" + str(top_divisive_words))\n",
        "\n",
        "\n",
        "    # Train and test the model on training set\n",
        "    model = fasttext.train_supervised(train_ft,\n",
        "                                      epoch=35,\n",
        "                                      )\n",
        "    train_performance = model.test('train.ft')\n",
        "    print(f\"Performance on train set \"\n",
        "          f\"({train_performance[0]} entries): \"\n",
        "          f\"\\t{train_performance[1]*100:.2f}% accuracy\")\n",
        "\n",
        "    model.save_model(\"model.bin\")\n",
        "\n",
        "    trained = time.time()\n",
        "    print(f\"Finished training in {trained - start:.2f} seconds\")\n",
        "\n",
        "    print(COUNT)\n",
        "\n",
        "    validate(model, VALIDATION_DIRS, Transcript)\n",
        "\n",
        "    print(COUNT)\n",
        "\n",
        "    print(f\"{VAL_MALE_ENTRIES} male partial conversations in validation\")\n",
        "    print(f\"{VAL_FEMALE_ENTRIES} female partial conversations in validation\")\n",
        "    print(f\"{VAL_DISCARDED_ENTRIES} discarded partial conversations in validation\")\n",
        "\n",
        "    # No peeking\n",
        "    test(model, TEST_DIRS, Transcript)\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Finished overall in {end - start:.2f} seconds\")"
      ],
      "metadata": {
        "id": "0iKl2bHLRDWx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4r7IL3cARHd-",
        "outputId": "dc5d539e-c40f-4e45-cf46-68712cf55600"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_DIRS: ['swda/sw00utt', 'swda/sw01utt', 'swda/sw02utt', 'swda/sw03utt', 'swda/sw04utt', 'swda/sw05utt', 'swda/sw06utt']\n",
            "Processing files in swda/sw00utt...\n",
            "Processing files in swda/sw01utt...\n",
            "Processing files in swda/sw02utt...\n",
            "Processing files in swda/sw03utt...\n",
            "Processing files in swda/sw04utt...\n",
            "Processing files in swda/sw05utt...\n",
            "Processing files in swda/sw06utt...\n",
            "Saved sentences and conversation sides to train.ft\n",
            "2499 male partial conversations in training\n",
            "3377 female partial conversations in training\n",
            "787 discarded partial conversations in training\n",
            "Total words: 1688207.0\n",
            "Unique words: 16588\n",
            "Words remaining: 1194\n",
            "Words filtered: 15394\n",
            "Most divisive words: dict_keys(['husband', 'recipe', 'sauce', 'wear', 'dinner', 'jean', 'salad', 'cooking', 'wonderful', 'brick', 'goodness', 'church', 'ooh', 'busy', 'girl', 'clothes', 'heat', 'dress', 'shell', 'painting', 'plano', 'scary', 'mother', 'woman', 'huhuh', 'dish', 'baby', 'stayed', 'daughter', 'fish'])\n",
            "Performance on train set (5089 entries): \t95.05% accuracy\n",
            "Finished training in 319.81 seconds\n",
            "140\n",
            "VALIDATION_DIRS: ['swda/sw07utt', 'swda/sw08utt', 'swda/sw09utt']\n",
            "Processing files in swda/sw07utt...\n",
            "Processing files in swda/sw08utt...\n",
            "Processing files in swda/sw09utt...\n",
            "Saved sentences and conversation sides to validation.ft\n",
            "Performance on validation set (2182 entries): \t81.30% accuracy\n",
            "194\n",
            "1085 male partial conversations in validation\n",
            "1651 female partial conversations in validation\n",
            "554 discarded partial conversations in validation\n",
            "TEST_DIRS: ['swda/sw10utt', 'swda/sw11utt', 'swda/sw12utt']\n",
            "Processing files in swda/sw10utt...\n",
            "Processing files in swda/sw11utt...\n",
            "Processing files in swda/sw12utt...\n",
            "Saved sentences and conversation sides to test.ft\n",
            "Performance on test set (1309 entries): \t80.60% accuracy\n",
            "Finished overall in 496.63 seconds\n"
          ]
        }
      ]
    }
  ]
}